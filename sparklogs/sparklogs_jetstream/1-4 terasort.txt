exouser@node-master:~$ spark-submit --master yarn --deploy-mode client --num-executors 4 --executor-cores 2 --executor-memory 1G /home/exouser/terasort_benchmark.py
25/04/28 13:48:36 INFO SparkContext: Running Spark version 3.4.1
25/04/28 13:48:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/28 13:48:36 INFO ResourceUtils: ==============================================================
25/04/28 13:48:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/28 13:48:36 INFO ResourceUtils: ==============================================================
25/04/28 13:48:36 INFO SparkContext: Submitted application: TeraSort
25/04/28 13:48:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/28 13:48:36 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
25/04/28 13:48:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/28 13:48:36 INFO SecurityManager: Changing view acls to: exouser
25/04/28 13:48:36 INFO SecurityManager: Changing modify acls to: exouser
25/04/28 13:48:36 INFO SecurityManager: Changing view acls groups to: 
25/04/28 13:48:36 INFO SecurityManager: Changing modify acls groups to: 
25/04/28 13:48:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: exouser; groups with view permissions: EMPTY; users with modify permissions: exouser; groups with modify permissions: EMPTY
25/04/28 13:48:36 INFO Utils: Successfully started service 'sparkDriver' on port 40773.
25/04/28 13:48:37 INFO SparkEnv: Registering MapOutputTracker
25/04/28 13:48:37 INFO SparkEnv: Registering BlockManagerMaster
25/04/28 13:48:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/28 13:48:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/28 13:48:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/28 13:48:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-94193926-1411-4f65-9e27-3172a1f66916
25/04/28 13:48:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/04/28 13:48:37 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/28 13:48:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/04/28 13:48:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/28 13:48:37 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at node-master/10.3.34.73:8032
25/04/28 13:48:38 INFO Configuration: resource-types.xml not found
25/04/28 13:48:38 INFO ResourceUtils: Unable to find 'resource-types.xml'.
25/04/28 13:48:38 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container)
25/04/28 13:48:38 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/04/28 13:48:38 INFO Client: Setting up container launch context for our AM
25/04/28 13:48:38 INFO Client: Setting up the launch environment for our AM container
25/04/28 13:48:38 INFO Client: Preparing resources for our AM container
25/04/28 13:48:38 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
25/04/28 13:48:40 INFO Client: Uploading resource file:/tmp/spark-f79a312f-944e-4af8-b102-e2772e558bf0/__spark_libs__6245621961676623471.zip -> hdfs://node-master:9000/user/exouser/.sparkStaging/application_1745862240057_0002/__spark_libs__6245621961676623471.zip
25/04/28 13:48:41 INFO Client: Uploading resource file:/home/exouser/spark/python/lib/pyspark.zip -> hdfs://node-master:9000/user/exouser/.sparkStaging/application_1745862240057_0002/pyspark.zip
25/04/28 13:48:41 INFO Client: Uploading resource file:/home/exouser/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://node-master:9000/user/exouser/.sparkStaging/application_1745862240057_0002/py4j-0.10.9.7-src.zip
25/04/28 13:48:41 INFO Client: Uploading resource file:/tmp/spark-f79a312f-944e-4af8-b102-e2772e558bf0/__spark_conf__4958710754998131107.zip -> hdfs://node-master:9000/user/exouser/.sparkStaging/application_1745862240057_0002/__spark_conf__.zip
25/04/28 13:48:41 INFO SecurityManager: Changing view acls to: exouser
25/04/28 13:48:41 INFO SecurityManager: Changing modify acls to: exouser
25/04/28 13:48:41 INFO SecurityManager: Changing view acls groups to: 
25/04/28 13:48:41 INFO SecurityManager: Changing modify acls groups to: 
25/04/28 13:48:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: exouser; groups with view permissions: EMPTY; users with modify permissions: exouser; groups with modify permissions: EMPTY
25/04/28 13:48:41 INFO Client: Submitting application application_1745862240057_0002 to ResourceManager
25/04/28 13:48:41 INFO YarnClientImpl: Submitted application application_1745862240057_0002
25/04/28 13:48:42 INFO Client: Application report for application_1745862240057_0002 (state: ACCEPTED)
25/04/28 13:48:42 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.default
	 start time: 1745862521886
	 final status: UNDEFINED
	 tracking URL: http://node-master:8088/proxy/application_1745862240057_0002/
	 user: exouser
25/04/28 13:48:43 INFO Client: Application report for application_1745862240057_0002 (state: ACCEPTED)
25/04/28 13:48:44 INFO Client: Application report for application_1745862240057_0002 (state: ACCEPTED)
25/04/28 13:48:45 INFO Client: Application report for application_1745862240057_0002 (state: ACCEPTED)
25/04/28 13:48:46 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> node-master, PROXY_URI_BASES -> http://node-master:8088/proxy/application_1745862240057_0002), /proxy/application_1745862240057_0002
25/04/28 13:48:46 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/04/28 13:48:46 INFO Client: Application report for application_1745862240057_0002 (state: RUNNING)
25/04/28 13:48:46 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.3.34.106
	 ApplicationMaster RPC port: -1
	 queue: root.default
	 start time: 1745862521886
	 final status: UNDEFINED
	 tracking URL: http://node-master:8088/proxy/application_1745862240057_0002/
	 user: exouser
25/04/28 13:48:46 INFO YarnClientSchedulerBackend: Application application_1745862240057_0002 has started running.
25/04/28 13:48:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40753.
25/04/28 13:48:46 INFO NettyBlockTransferService: Server created on node-master:40753
25/04/28 13:48:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/28 13:48:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, node-master, 40753, None)
25/04/28 13:48:46 INFO BlockManagerMasterEndpoint: Registering block manager node-master:40753 with 434.4 MiB RAM, BlockManagerId(driver, node-master, 40753, None)
25/04/28 13:48:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, node-master, 40753, None)
25/04/28 13:48:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, node-master, 40753, None)
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:47 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:48:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.3.34.151:55818) with ID 1,  ResourceProfileId 0
25/04/28 13:48:51 INFO BlockManagerMasterEndpoint: Registering block manager node-worker3:37081 with 434.4 MiB RAM, BlockManagerId(1, node-worker3, 37081, None)
25/04/28 13:49:07 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
25/04/28 13:49:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/04/28 13:49:07 INFO SharedState: Warehouse path is 'file:/home/exouser/spark-warehouse'.
25/04/28 13:49:07 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:49:07 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:49:07 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:49:07 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:49:07 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/04/28 13:49:08 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.
25/04/28 13:49:09 INFO FileSourceStrategy: Pushed Filters: 
25/04/28 13:49:10 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/28 13:49:10 INFO CodeGenerator: Code generated in 139.090663 ms
25/04/28 13:49:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.0 KiB, free 434.2 MiB)
25/04/28 13:49:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 434.2 MiB)
25/04/28 13:49:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on node-master:40753 (size: 35.4 KiB, free: 434.4 MiB)
25/04/28 13:49:10 INFO SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0
25/04/28 13:49:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
25/04/28 13:49:10 INFO DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/04/28 13:49:10 INFO DAGScheduler: Got map stage job 0 (count at NativeMethodAccessorImpl.java:0) with 8 output partitions
25/04/28 13:49:10 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:10 INFO DAGScheduler: Parents of final stage: List()
25/04/28 13:49:10 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 434.2 MiB)
25/04/28 13:49:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.1 MiB)
25/04/28 13:49:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on node-master:40753 (size: 7.1 KiB, free: 434.4 MiB)
25/04/28 13:49:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:10 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
25/04/28 13:49:10 INFO YarnScheduler: Adding task set 0.0 with 8 tasks resource profile 0
25/04/28 13:49:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0) (node-worker3, executor 1, partition 1, NODE_LOCAL, 7919 bytes) 
25/04/28 13:49:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on node-worker3:37081 (size: 7.1 KiB, free: 434.4 MiB)
25/04/28 13:49:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on node-worker3:37081 (size: 35.4 KiB, free: 434.4 MiB)
25/04/28 13:49:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 1697 ms on node-worker3 (executor 1) (1/8)
25/04/28 13:49:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 1) (node-worker3, executor 1, partition 3, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:14 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 2) (node-worker3, executor 1, partition 4, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:14 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 3) (node-worker3, executor 1, partition 5, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 1) in 395 ms on node-worker3 (executor 1) (2/8)
25/04/28 13:49:14 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 4) (node-worker3, executor 1, partition 7, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:14 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 2) in 446 ms on node-worker3 (executor 1) (3/8)
25/04/28 13:49:15 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 5) (node-worker3, executor 1, partition 2, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:15 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 3) in 389 ms on node-worker3 (executor 1) (4/8)
25/04/28 13:49:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 6) (node-worker3, executor 1, partition 0, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:15 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 4) in 344 ms on node-worker3 (executor 1) (5/8)
25/04/28 13:49:15 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 7) (node-worker3, executor 1, partition 6, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 6) in 226 ms on node-worker3 (executor 1) (6/8)
25/04/28 13:49:15 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 5) in 248 ms on node-worker3 (executor 1) (7/8)
25/04/28 13:49:15 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 7) in 209 ms on node-worker3 (executor 1) (8/8)
25/04/28 13:49:15 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/28 13:49:15 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 4.938 s
25/04/28 13:49:15 INFO DAGScheduler: looking for newly runnable stages
25/04/28 13:49:15 INFO DAGScheduler: running: Set()
25/04/28 13:49:15 INFO DAGScheduler: waiting: Set()
25/04/28 13:49:15 INFO DAGScheduler: failed: Set()
25/04/28 13:49:15 INFO CodeGenerator: Code generated in 12.186392 ms
25/04/28 13:49:15 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
25/04/28 13:49:15 INFO DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/28 13:49:15 INFO DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
25/04/28 13:49:15 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 434.1 MiB)
25/04/28 13:49:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.1 MiB)
25/04/28 13:49:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on node-master:40753 (size: 5.8 KiB, free: 434.4 MiB)
25/04/28 13:49:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/28 13:49:15 INFO YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0
25/04/28 13:49:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (node-worker3, executor 1, partition 0, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on node-worker3:37081 (size: 5.8 KiB, free: 434.4 MiB)
25/04/28 13:49:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.3.34.151:55818
25/04/28 13:49:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 157 ms on node-worker3 (executor 1) (1/1)
25/04/28 13:49:15 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/04/28 13:49:15 INFO DAGScheduler: ResultStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.176 s
25/04/28 13:49:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/28 13:49:15 INFO YarnScheduler: Killing all running tasks in stage 2: Stage finished
25/04/28 13:49:15 INFO DAGScheduler: Job 1 finished: count at NativeMethodAccessorImpl.java:0, took 0.193713 s
25/04/28 13:49:16 INFO FileSourceStrategy: Pushed Filters: 
25/04/28 13:49:16 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/28 13:49:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 201.0 KiB, free 433.9 MiB)
25/04/28 13:49:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 433.9 MiB)
25/04/28 13:49:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on node-master:40753 (size: 35.4 KiB, free: 434.3 MiB)
25/04/28 13:49:16 INFO SparkContext: Created broadcast 3 from text at NativeMethodAccessorImpl.java:0
25/04/28 13:49:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
25/04/28 13:49:16 INFO CodeGenerator: Code generated in 15.137701 ms
25/04/28 13:49:16 INFO SparkContext: Starting job: text at NativeMethodAccessorImpl.java:0
25/04/28 13:49:16 INFO DAGScheduler: Got job 2 (text at NativeMethodAccessorImpl.java:0) with 8 output partitions
25/04/28 13:49:16 INFO DAGScheduler: Final stage: ResultStage 3 (text at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:16 INFO DAGScheduler: Parents of final stage: List()
25/04/28 13:49:16 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at text at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.6 KiB, free 433.9 MiB)
25/04/28 13:49:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)
25/04/28 13:49:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on node-master:40753 (size: 5.9 KiB, free: 434.3 MiB)
25/04/28 13:49:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:16 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at text at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
25/04/28 13:49:16 INFO YarnScheduler: Adding task set 3.0 with 8 tasks resource profile 0
25/04/28 13:49:16 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 9) (node-worker3, executor 1, partition 1, NODE_LOCAL, 7930 bytes) 
25/04/28 13:49:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on node-worker3:37081 (size: 5.9 KiB, free: 434.3 MiB)
25/04/28 13:49:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on node-worker3:37081 (size: 35.4 KiB, free: 434.3 MiB)
25/04/28 13:49:17 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 9) in 974 ms on node-worker3 (executor 1) (1/8)
25/04/28 13:49:19 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 10) (node-worker3, executor 1, partition 3, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:19 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 11) (node-worker3, executor 1, partition 4, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:20 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 12) (node-worker3, executor 1, partition 5, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:20 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 10) in 891 ms on node-worker3 (executor 1) (2/8)
25/04/28 13:49:20 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 13) (node-worker3, executor 1, partition 7, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:20 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 11) in 907 ms on node-worker3 (executor 1) (3/8)
25/04/28 13:49:20 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 14) (node-worker3, executor 1, partition 2, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:20 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 13) in 654 ms on node-worker3 (executor 1) (4/8)
25/04/28 13:49:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 15) (node-worker3, executor 1, partition 0, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:21 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 12) in 911 ms on node-worker3 (executor 1) (5/8)
25/04/28 13:49:21 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 16) (node-worker3, executor 1, partition 6, RACK_LOCAL, 7930 bytes) 
25/04/28 13:49:21 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 14) in 904 ms on node-worker3 (executor 1) (6/8)
25/04/28 13:49:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 15) in 901 ms on node-worker3 (executor 1) (7/8)
25/04/28 13:49:22 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 16) in 859 ms on node-worker3 (executor 1) (8/8)
25/04/28 13:49:22 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
25/04/28 13:49:22 INFO DAGScheduler: ResultStage 3 (text at NativeMethodAccessorImpl.java:0) finished in 6.625 s
25/04/28 13:49:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/28 13:49:22 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished
25/04/28 13:49:22 INFO DAGScheduler: Job 2 finished: text at NativeMethodAccessorImpl.java:0, took 6.629349 s
25/04/28 13:49:22 INFO DAGScheduler: Registering RDD 12 (text at NativeMethodAccessorImpl.java:0) as input to shuffle 1
25/04/28 13:49:22 INFO DAGScheduler: Got map stage job 3 (text at NativeMethodAccessorImpl.java:0) with 8 output partitions
25/04/28 13:49:22 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (text at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:22 INFO DAGScheduler: Parents of final stage: List()
25/04/28 13:49:22 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:22 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at text at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 23.1 KiB, free 433.9 MiB)
25/04/28 13:49:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.9 KiB, free 433.9 MiB)
25/04/28 13:49:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on node-master:40753 (size: 9.9 KiB, free: 434.3 MiB)
25/04/28 13:49:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:22 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at text at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
25/04/28 13:49:22 INFO YarnScheduler: Adding task set 4.0 with 8 tasks resource profile 0
25/04/28 13:49:22 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17) (node-worker3, executor 1, partition 1, NODE_LOCAL, 7919 bytes) 
25/04/28 13:49:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on node-worker3:37081 (size: 9.9 KiB, free: 434.3 MiB)
25/04/28 13:49:24 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 1469 ms on node-worker3 (executor 1) (1/8)
25/04/28 13:49:26 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 18) (node-worker3, executor 1, partition 3, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:26 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 19) (node-worker3, executor 1, partition 4, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:27 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 20) (node-worker3, executor 1, partition 5, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:27 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 18) in 1448 ms on node-worker3 (executor 1) (2/8)
25/04/28 13:49:27 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 21) (node-worker3, executor 1, partition 7, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:27 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 19) in 1453 ms on node-worker3 (executor 1) (3/8)
25/04/28 13:49:28 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 22) (node-worker3, executor 1, partition 2, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:28 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 21) in 1057 ms on node-worker3 (executor 1) (4/8)
25/04/28 13:49:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 23) (node-worker3, executor 1, partition 0, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:29 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 20) in 1423 ms on node-worker3 (executor 1) (5/8)
25/04/28 13:49:30 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 24) (node-worker3, executor 1, partition 6, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:30 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 22) in 1409 ms on node-worker3 (executor 1) (6/8)
25/04/28 13:49:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 23) in 1446 ms on node-worker3 (executor 1) (7/8)
25/04/28 13:49:31 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 24) in 1045 ms on node-worker3 (executor 1) (8/8)
25/04/28 13:49:31 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/04/28 13:49:31 INFO DAGScheduler: ShuffleMapStage 4 (text at NativeMethodAccessorImpl.java:0) finished in 8.553 s
25/04/28 13:49:31 INFO DAGScheduler: looking for newly runnable stages
25/04/28 13:49:31 INFO DAGScheduler: running: Set()
25/04/28 13:49:31 INFO DAGScheduler: waiting: Set()
25/04/28 13:49:31 INFO DAGScheduler: failed: Set()
25/04/28 13:49:31 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 67108864, minimum partition size: 1048576
25/04/28 13:49:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/28 13:49:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/28 13:49:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
25/04/28 13:49:31 INFO CodeGenerator: Code generated in 12.123904 ms
25/04/28 13:49:31 INFO SparkContext: Starting job: text at NativeMethodAccessorImpl.java:0
25/04/28 13:49:31 INFO DAGScheduler: Got job 4 (text at NativeMethodAccessorImpl.java:0) with 10 output partitions
25/04/28 13:49:31 INFO DAGScheduler: Final stage: ResultStage 6 (text at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
25/04/28 13:49:31 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:31 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[15] at text at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 229.5 KiB, free 433.6 MiB)
25/04/28 13:49:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.4 KiB, free 433.5 MiB)
25/04/28 13:49:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on node-master:40753 (size: 85.4 KiB, free: 434.2 MiB)
25/04/28 13:49:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:31 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 6 (MapPartitionsRDD[15] at text at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
25/04/28 13:49:31 INFO YarnScheduler: Adding task set 6.0 with 10 tasks resource profile 0
25/04/28 13:49:31 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 25) (node-worker3, executor 1, partition 0, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:31 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 26) (node-worker3, executor 1, partition 1, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on node-worker3:37081 (size: 85.4 KiB, free: 434.2 MiB)
25/04/28 13:49:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.3.34.151:55818
25/04/28 13:49:33 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 27) (node-worker3, executor 1, partition 2, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:33 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 26) in 2079 ms on node-worker3 (executor 1) (1/10)
25/04/28 13:49:34 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 28) (node-worker3, executor 1, partition 3, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:34 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 25) in 2649 ms on node-worker3 (executor 1) (2/10)
25/04/28 13:49:35 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 29) (node-worker3, executor 1, partition 4, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:35 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 27) in 1797 ms on node-worker3 (executor 1) (3/10)
25/04/28 13:49:35 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 30) (node-worker3, executor 1, partition 5, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:35 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 28) in 1314 ms on node-worker3 (executor 1) (4/10)
25/04/28 13:49:37 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 31) (node-worker3, executor 1, partition 6, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:37 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 29) in 1761 ms on node-worker3 (executor 1) (5/10)
25/04/28 13:49:37 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 32) (node-worker3, executor 1, partition 7, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:37 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 30) in 1744 ms on node-worker3 (executor 1) (6/10)
25/04/28 13:49:38 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 33) (node-worker3, executor 1, partition 8, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:38 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 31) in 1747 ms on node-worker3 (executor 1) (7/10)
25/04/28 13:49:39 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 34) (node-worker3, executor 1, partition 9, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:39 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 32) in 1820 ms on node-worker3 (executor 1) (8/10)
25/04/28 13:49:39 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 34) in 106 ms on node-worker3 (executor 1) (9/10)
25/04/28 13:49:40 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 33) in 1693 ms on node-worker3 (executor 1) (10/10)
25/04/28 13:49:40 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
25/04/28 13:49:40 INFO DAGScheduler: ResultStage 6 (text at NativeMethodAccessorImpl.java:0) finished in 9.103 s
25/04/28 13:49:40 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/28 13:49:40 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished
25/04/28 13:49:40 INFO DAGScheduler: Job 4 finished: text at NativeMethodAccessorImpl.java:0, took 9.117865 s
25/04/28 13:49:40 INFO FileFormatWriter: Start to commit write Job 32660e30-ce66-4c60-bcf3-ac21bdb797a8.
25/04/28 13:49:40 INFO FileFormatWriter: Write Job 32660e30-ce66-4c60-bcf3-ac21bdb797a8 committed. Elapsed time: 58 ms.
25/04/28 13:49:40 INFO FileFormatWriter: Finished processing stats for write job 32660e30-ce66-4c60-bcf3-ac21bdb797a8.
25/04/28 13:49:40 INFO FileSourceStrategy: Pushed Filters: 
25/04/28 13:49:40 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/28 13:49:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 201.0 KiB, free 433.3 MiB)
25/04/28 13:49:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 433.3 MiB)
25/04/28 13:49:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on node-master:40753 (size: 35.4 KiB, free: 434.2 MiB)
25/04/28 13:49:40 INFO SparkContext: Created broadcast 7 from count at NativeMethodAccessorImpl.java:0
25/04/28 13:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
25/04/28 13:49:40 INFO DAGScheduler: Registering RDD 19 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2
25/04/28 13:49:40 INFO DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 8 output partitions
25/04/28 13:49:40 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:40 INFO DAGScheduler: Parents of final stage: List()
25/04/28 13:49:40 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:40 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 14.3 KiB, free 433.3 MiB)
25/04/28 13:49:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 433.3 MiB)
25/04/28 13:49:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on node-master:40753 (size: 7.1 KiB, free: 434.2 MiB)
25/04/28 13:49:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:40 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
25/04/28 13:49:40 INFO YarnScheduler: Adding task set 7.0 with 8 tasks resource profile 0
25/04/28 13:49:40 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 35) (node-worker3, executor 1, partition 1, NODE_LOCAL, 7919 bytes) 
25/04/28 13:49:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on node-worker3:37081 (size: 7.1 KiB, free: 434.2 MiB)
25/04/28 13:49:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on node-worker3:37081 (size: 35.4 KiB, free: 434.2 MiB)
25/04/28 13:49:40 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 35) in 223 ms on node-worker3 (executor 1) (1/8)
25/04/28 13:49:44 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 36) (node-worker3, executor 1, partition 3, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:44 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 37) (node-worker3, executor 1, partition 4, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:44 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 38) (node-worker3, executor 1, partition 5, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:44 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 37) in 248 ms on node-worker3 (executor 1) (2/8)
25/04/28 13:49:44 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 39) (node-worker3, executor 1, partition 7, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:44 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 36) in 258 ms on node-worker3 (executor 1) (3/8)
25/04/28 13:49:44 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 40) (node-worker3, executor 1, partition 2, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:44 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 39) in 263 ms on node-worker3 (executor 1) (4/8)
25/04/28 13:49:45 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 41) (node-worker3, executor 1, partition 0, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:45 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 38) in 425 ms on node-worker3 (executor 1) (5/8)
25/04/28 13:49:45 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 42) (node-worker3, executor 1, partition 6, RACK_LOCAL, 7919 bytes) 
25/04/28 13:49:45 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 40) in 392 ms on node-worker3 (executor 1) (6/8)
25/04/28 13:49:45 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 41) in 427 ms on node-worker3 (executor 1) (7/8)
25/04/28 13:49:45 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 42) in 243 ms on node-worker3 (executor 1) (8/8)
25/04/28 13:49:45 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
25/04/28 13:49:45 INFO DAGScheduler: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 4.844 s
25/04/28 13:49:45 INFO DAGScheduler: looking for newly runnable stages
25/04/28 13:49:45 INFO DAGScheduler: running: Set()
25/04/28 13:49:45 INFO DAGScheduler: waiting: Set()
25/04/28 13:49:45 INFO DAGScheduler: failed: Set()
25/04/28 13:49:45 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
25/04/28 13:49:45 INFO DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/28 13:49:45 INFO DAGScheduler: Final stage: ResultStage 9 (count at NativeMethodAccessorImpl.java:0)
25/04/28 13:49:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
25/04/28 13:49:45 INFO DAGScheduler: Missing parents: List()
25/04/28 13:49:45 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/28 13:49:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.1 KiB, free 433.3 MiB)
25/04/28 13:49:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.3 MiB)
25/04/28 13:49:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on node-master:40753 (size: 5.8 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
25/04/28 13:49:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/28 13:49:45 INFO YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0
25/04/28 13:49:45 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 43) (node-worker3, executor 1, partition 0, NODE_LOCAL, 7374 bytes) 
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on node-master:40753 in memory (size: 35.4 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on node-worker3:37081 (size: 5.8 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on node-worker3:37081 in memory (size: 35.4 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.3.34.151:55818
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on node-master:40753 in memory (size: 35.4 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 43) in 24 ms on node-worker3 (executor 1) (1/1)
25/04/28 13:49:45 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on node-worker3:37081 in memory (size: 35.4 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO DAGScheduler: ResultStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.039 s
25/04/28 13:49:45 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/28 13:49:45 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished
25/04/28 13:49:45 INFO DAGScheduler: Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.045018 s
========== Spark TeraSort Benchmark ==========
Number of Workers: 4
Wall Time: 38.08 seconds
Input Rows: 781481
Output Rows: 781481
Estimated Input Size: 74.53 MB
Estimated Output Size: 74.53 MB
Shuffle Throughput: 3.91 MB/s
Aggregate Resource Utilization: 152314.37 vcore-ms (approx)
Memory Utilization per Worker: 1024.00 MB
HDFS I/O Throughput per Worker: 0.98 MB/s
CPU Efficiency: N/A
Killed Tasks per Worker: N/A
==============================================
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on node-master:40753 in memory (size: 5.9 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on node-worker3:37081 in memory (size: 5.9 KiB, free: 434.2 MiB)
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on node-master:40753 in memory (size: 9.9 KiB, free: 434.3 MiB)
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on node-worker3:37081 in memory (size: 9.9 KiB, free: 434.3 MiB)
25/04/28 13:49:45 INFO SparkUI: Stopped Spark web UI at http://node-master:4040
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on node-master:40753 in memory (size: 85.4 KiB, free: 434.3 MiB)
25/04/28 13:49:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on node-worker3:37081 in memory (size: 85.4 KiB, free: 434.3 MiB)
25/04/28 13:49:45 INFO YarnClientSchedulerBackend: Interrupting monitor thread
25/04/28 13:49:45 INFO YarnClientSchedulerBackend: Shutting down all executors
25/04/28 13:49:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/04/28 13:49:45 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
25/04/28 13:49:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/28 13:49:45 INFO MemoryStore: MemoryStore cleared
25/04/28 13:49:45 INFO BlockManager: BlockManager stopped
25/04/28 13:49:45 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/28 13:49:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/28 13:49:45 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.3.34.151:55818 is closed
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@72b79216 rejected from java.util.concurrent.ThreadPoolExecutor@2c9b7cc0[Shutting down, pool size = 17, active threads = 0, queued tasks = 0, completed tasks = 26]
	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)
	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)
	at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.tryFailure(Promise.scala:112)
	at scala.concurrent.Promise.tryFailure$(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)
	at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$6(NettyRpcEnv.scala:245)
	at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$6$adapted(NettyRpcEnv.scala:245)
	at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:86)
	at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:118)
	at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)
	at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasksFrom(SingleThreadEventExecutor.java:426)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:375)
	at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:763)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:596)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
25/04/28 13:49:45 INFO SparkContext: Successfully stopped SparkContext
25/04/28 13:49:46 INFO ShutdownHookManager: Shutdown hook called
25/04/28 13:49:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-f79a312f-944e-4af8-b102-e2772e558bf0
25/04/28 13:49:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-f79a312f-944e-4af8-b102-e2772e558bf0/pyspark-0407115c-8ae0-454d-880c-24d8f9a2707e
25/04/28 13:49:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-fbba5649-fd85-463f-b0d2-45766c145b77
exouser@node-master:~$ 
